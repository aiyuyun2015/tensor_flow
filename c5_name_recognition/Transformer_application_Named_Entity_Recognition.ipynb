{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Transformer-Network-Application:-Named-Entity-Recognition\" data-toc-modified-id=\"Transformer-Network-Application:-Named-Entity-Recognition-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Transformer Network Application: Named-Entity Recognition</a></div><div class=\"lev2 toc-item\"><a href=\"#Packages\" data-toc-modified-id=\"Packages-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Packages</a></div><div class=\"lev2 toc-item\"><a href=\"#1---Named-Entity-Recogniton-to-Process-Resumes\" data-toc-modified-id=\"1---Named-Entity-Recogniton-to-Process-Resumes-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>1 - Named-Entity Recogniton to Process Resumes</a></div><div class=\"lev3 toc-item\"><a href=\"#1.1---Dataset-Cleaning\" data-toc-modified-id=\"1.1---Dataset-Cleaning-121\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>1.1 - Dataset Cleaning</a></div><div class=\"lev3 toc-item\"><a href=\"#1.2---Padding-and-Generating-Tags\" data-toc-modified-id=\"1.2---Padding-and-Generating-Tags-122\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>1.2 - Padding and Generating Tags</a></div><div class=\"lev3 toc-item\"><a href=\"#1.3---Tokenize-and-Align-Labels-with-ðŸ¤—-Library\" data-toc-modified-id=\"1.3---Tokenize-and-Align-Labels-with-ðŸ¤—-Library-123\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>1.3 - Tokenize and Align Labels with ðŸ¤— Library</a></div><div class=\"lev3 toc-item\"><a href=\"#Exercise-1---tokenize_and_align_labels\" data-toc-modified-id=\"Exercise-1---tokenize_and_align_labels-124\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>Exercise 1 - tokenize_and_align_labels</a></div><div class=\"lev2 toc-item\"><a href=\"#Star-From-Here\" data-toc-modified-id=\"Star-From-Here-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Star From Here</a></div><div class=\"lev3 toc-item\"><a href=\"#1.4---Optimization\" data-toc-modified-id=\"1.4---Optimization-131\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>1.4 - Optimization</a></div><div class=\"lev3 toc-item\"><a href=\"#Congratulations!\" data-toc-modified-id=\"Congratulations!-132\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Congratulations!</a></div><div class=\"lev4 toc-item\"><a href=\"#Here's-what-you-should-remember\" data-toc-modified-id=\"Here's-what-you-should-remember-1321\"><span class=\"toc-item-num\">1.3.2.1&nbsp;&nbsp;</span>Here's what you should remember</a></div><div class=\"lev2 toc-item\"><a href=\"#Appendix-(Nothing-Useful)\" data-toc-modified-id=\"Appendix-(Nothing-Useful)-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Appendix (Nothing Useful)</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Network Application: Named-Entity Recognition\n",
    "\n",
    "Welcome to Week 4's first ungraded lab. In this notebook you'll explore one application of the transformer architecture that you built in the previous assignment.\n",
    "\n",
    "**After this assignment you'll be able to**:\n",
    "\n",
    "* Use tokenizers and pre-trained models from the HuggingFace Library.\n",
    "* Fine-tune a pre-trained transformer model for Named-Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Packages](#0)\n",
    "- [1 - Named-Entity Recogniton to Process Resumes](#1)\n",
    "    - [1.1 - Data Cleaning](#1-1)\n",
    "    - [1.2 - Padding and Generating Tags](#1-2)\n",
    "    - [1.3 - Tokenize and Align Labels with ðŸ¤— Library](#1-3)\n",
    "        - [Exercise 1 - tokenize_and_align_labels](#ex-1)\n",
    "    - [1.4 - Optimization](#1-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Packages\n",
    "\n",
    "Run the following cell to load the packages you'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Named-Entity Recogniton to Process Resumes\n",
    "\n",
    "When faced with a large amount of unstructured text data, named-entity recognition (NER) can help you detect and classify important information in your dataset. For instance, in the running example \"Jane vists Africa in September\", NER would help you detect \"Jane\", \"Africa\", and \"September\" as named-entities and classify them as person, location, and time. \n",
    "\n",
    "* You will use a variation of the Transformer model you built in the last assignment to process a large dataset of resumes.\n",
    "* You will find and classify relavent information such as the companies the applicant worked at, skills, type of degree, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-1'></a>\n",
    "### 1.1 - Dataset Cleaning\n",
    "\n",
    "In this assignment you will optimize a Transformer model on a dataset of resumes. Take a look at how the data you will be working with are structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_json(\"ner.json\", lines=True)\n",
    "df_data = df_data.drop(['extras'], axis=1)\n",
    "df_data['content'] = df_data['content'].str.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhishek Jha Application Development Associate...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afreen Jamadar Active member of IIIT Committee...</td>\n",
       "      <td>[{'label': ['Email Address'], 'points': [{'sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akhil Yadav Polemaina Hyderabad, Telangana - E...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alok Khandai Operational Analyst (SQL DBA) Eng...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ananya Chavan lecturer - oracle tutorials  Mum...</td>\n",
       "      <td>[{'label': ['Degree'], 'points': [{'start': 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Abhishek Jha Application Development Associate...   \n",
       "1  Afreen Jamadar Active member of IIIT Committee...   \n",
       "2  Akhil Yadav Polemaina Hyderabad, Telangana - E...   \n",
       "3  Alok Khandai Operational Analyst (SQL DBA) Eng...   \n",
       "4  Ananya Chavan lecturer - oracle tutorials  Mum...   \n",
       "\n",
       "                                          annotation  \n",
       "0  [{'label': ['Skills'], 'points': [{'start': 12...  \n",
       "1  [{'label': ['Email Address'], 'points': [{'sta...  \n",
       "2  [{'label': ['Skills'], 'points': [{'start': 37...  \n",
       "3  [{'label': ['Skills'], 'points': [{'start': 80...  \n",
       "4  [{'label': ['Degree'], 'points': [{'start': 20...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understand Our Annotation Data (Tagged Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The df_data['annotation'] includes the tagged data, with a pair ('point'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' â€¢ Programming language: C, C++, Java â€¢ Oracle PeopleSoft â€¢ Internet Of Things â€¢ Machine Learning â€¢ Database Management System â€¢ Computer Networks â€¢ Operating System worked on: Linux, Windows, Mac  Non - Technical Skills  â€¢ Honest and Hard-Working â€¢ Tolerant and Flexible to Different Situations â€¢ Polite and Calm â€¢ Team-Playe'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.iloc[0]['content'][1295:1621]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': ['Skills'],\n",
       "  'points': [{'start': 1295,\n",
       "    'end': 1621,\n",
       "    'text': '\\nâ€¢ Programming language: C, C++, Java\\nâ€¢ Oracle PeopleSoft\\nâ€¢ Internet Of Things\\nâ€¢ Machine Learning\\nâ€¢ Database Management System\\nâ€¢ Computer Networks\\nâ€¢ Operating System worked on: Linux, Windows, Mac\\n\\nNon - Technical Skills\\n\\nâ€¢ Honest and Hard-Working\\nâ€¢ Tolerant and Flexible to Different Situations\\nâ€¢ Polite and Calm\\nâ€¢ Team-Player'}]},\n",
       " {'label': ['Skills'],\n",
       "  'points': [{'start': 993,\n",
       "    'end': 1153,\n",
       "    'text': 'C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year),\\nDatabase Management System (Less than 1 year), Java (Less than 1 year)'}]},\n",
       " {'label': ['College Name'],\n",
       "  'points': [{'start': 939, 'end': 956, 'text': 'Kendriya Vidyalaya'}]},\n",
       " {'label': ['College Name'],\n",
       "  'points': [{'start': 883, 'end': 904, 'text': 'Woodbine modern school'}]},\n",
       " {'label': ['Graduation Year'],\n",
       "  'points': [{'start': 856, 'end': 860, 'text': '2017\\n'}]},\n",
       " {'label': ['College Name'],\n",
       "  'points': [{'start': 771,\n",
       "    'end': 813,\n",
       "    'text': 'B.v.b college of engineering and technology'}]},\n",
       " {'label': ['Designation'],\n",
       "  'points': [{'start': 727,\n",
       "    'end': 769,\n",
       "    'text': 'B.E in Information science and engineering\\n'}]},\n",
       " {'label': ['Companies worked at'],\n",
       "  'points': [{'start': 407, 'end': 415, 'text': 'Accenture'}]},\n",
       " {'label': ['Designation'],\n",
       "  'points': [{'start': 372,\n",
       "    'end': 404,\n",
       "    'text': 'Application Development Associate'}]},\n",
       " {'label': ['Email Address'],\n",
       "  'points': [{'start': 95,\n",
       "    'end': 145,\n",
       "    'text': 'Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\\n'}]},\n",
       " {'label': ['Location'],\n",
       "  'points': [{'start': 60, 'end': 68, 'text': 'Bengaluru'}]},\n",
       " {'label': ['Companies worked at'],\n",
       "  'points': [{'start': 49, 'end': 57, 'text': 'Accenture'}]},\n",
       " {'label': ['Designation'],\n",
       "  'points': [{'start': 13,\n",
       "    'end': 45,\n",
       "    'text': 'Application Development Associate'}]},\n",
       " {'label': ['Name'],\n",
       "  'points': [{'start': 0, 'end': 11, 'text': 'Abhishek Jha'}]}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.iloc[0]['annotation']\n",
    "# a list of dictionary, inside each ditionary it has lables=[skill, name, email, etc], points:[indicate the positions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Skills']\n",
      "['Skills']\n",
      "['College Name']\n",
      "['College Name']\n",
      "['Graduation Year']\n",
      "['College Name']\n",
      "['Designation']\n",
      "['Companies worked at']\n",
      "['Designation']\n",
      "['Email Address']\n",
      "['Location']\n",
      "['Companies worked at']\n",
      "['Designation']\n",
      "['Name']\n"
     ]
    }
   ],
   "source": [
    "for i in df_data.iloc[0]['annotation']:\n",
    "    print(i['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Continue Our Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeIntervals(intervals):\n",
    "    sorted_by_lower_bound = sorted(intervals, key=lambda tup: tup[0])\n",
    "    merged = []\n",
    "\n",
    "    for higher in sorted_by_lower_bound:\n",
    "        if not merged:\n",
    "            merged.append(higher)\n",
    "        else:\n",
    "            lower = merged[-1]\n",
    "            if higher[0] <= lower[1]:\n",
    "                if lower[2] is higher[2]:\n",
    "                    upper_bound = max(lower[1], higher[1])\n",
    "                    merged[-1] = (lower[0], upper_bound, lower[2])\n",
    "                else:\n",
    "                    if lower[1] > higher[1]:\n",
    "                        merged[-1] = lower\n",
    "                    else:\n",
    "                        merged[-1] = (lower[0], higher[1], higher[2])\n",
    "            else:\n",
    "                merged.append(higher)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(df):\n",
    "    \n",
    "    entities = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        entity = []\n",
    "    \n",
    "        for annot in df['annotation'][i]:\n",
    "            try:\n",
    "                ent = annot['label'][0]\n",
    "                start = annot['points'][0]['start']\n",
    "                end = annot['points'][0]['end'] + 1\n",
    "                entity.append((start, end, ent))\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "        entity = mergeIntervals(entity)\n",
    "        entities.append(entity)\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>annotation</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhishek Jha Application Development Associate...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 12...</td>\n",
       "      <td>[(0, 12, Name), (13, 46, Designation), (49, 58...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afreen Jamadar Active member of IIIT Committee...</td>\n",
       "      <td>[{'label': ['Email Address'], 'points': [{'sta...</td>\n",
       "      <td>[(0, 14, Name), (62, 68, Location), (104, 148,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akhil Yadav Polemaina Hyderabad, Telangana - E...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 37...</td>\n",
       "      <td>[(0, 21, Name), (22, 31, Location), (65, 117, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alok Khandai Operational Analyst (SQL DBA) Eng...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 80...</td>\n",
       "      <td>[(0, 12, Name), (13, 51, Designation), (54, 60...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ananya Chavan lecturer - oracle tutorials  Mum...</td>\n",
       "      <td>[{'label': ['Degree'], 'points': [{'start': 20...</td>\n",
       "      <td>[(0, 13, Name), (14, 22, Designation), (24, 41...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Abhishek Jha Application Development Associate...   \n",
       "1  Afreen Jamadar Active member of IIIT Committee...   \n",
       "2  Akhil Yadav Polemaina Hyderabad, Telangana - E...   \n",
       "3  Alok Khandai Operational Analyst (SQL DBA) Eng...   \n",
       "4  Ananya Chavan lecturer - oracle tutorials  Mum...   \n",
       "\n",
       "                                          annotation  \\\n",
       "0  [{'label': ['Skills'], 'points': [{'start': 12...   \n",
       "1  [{'label': ['Email Address'], 'points': [{'sta...   \n",
       "2  [{'label': ['Skills'], 'points': [{'start': 37...   \n",
       "3  [{'label': ['Skills'], 'points': [{'start': 80...   \n",
       "4  [{'label': ['Degree'], 'points': [{'start': 20...   \n",
       "\n",
       "                                            entities  \n",
       "0  [(0, 12, Name), (13, 46, Designation), (49, 58...  \n",
       "1  [(0, 14, Name), (62, 68, Location), (104, 148,...  \n",
       "2  [(0, 21, Name), (22, 31, Location), (65, 117, ...  \n",
       "3  [(0, 12, Name), (13, 51, Designation), (54, 60...  \n",
       "4  [(0, 13, Name), (14, 22, Designation), (24, 41...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['entities'] = get_entities(df_data)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 12, 'Name'),\n",
       " (13, 46, 'Designation'),\n",
       " (49, 58, 'Companies worked at'),\n",
       " (60, 69, 'Location'),\n",
       " (95, 146, 'Email Address'),\n",
       " (372, 405, 'Designation'),\n",
       " (407, 416, 'Companies worked at'),\n",
       " (727, 770, 'Designation'),\n",
       " (771, 814, 'College Name'),\n",
       " (856, 861, 'Graduation Year'),\n",
       " (883, 905, 'College Name'),\n",
       " (939, 957, 'College Name'),\n",
       " (993, 1154, 'Skills'),\n",
       " (1295, 1622, 'Skills')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['entities'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    try:\n",
    "        training_data = []\n",
    "        lines=[]\n",
    "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content'].replace(\"\\n\", \" \")\n",
    "            entities = []\n",
    "            data_annotations = data['annotation']\n",
    "            if data_annotations is not None:\n",
    "                for annotation in data_annotations:\n",
    "                    #only a single point in text annotation.\n",
    "                    point = annotation['points'][0]\n",
    "                    labels = annotation['label']\n",
    "                    # handle both list of labels or a single label.\n",
    "                    if not isinstance(labels, list):\n",
    "                        labels = [labels]\n",
    "\n",
    "                    for label in labels:\n",
    "                        point_start = point['start']\n",
    "                        point_end = point['end']\n",
    "                        point_text = point['text']\n",
    "                        \n",
    "                        lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
    "                        rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
    "                        if lstrip_diff != 0:\n",
    "                            point_start = point_start + lstrip_diff\n",
    "                        if rstrip_diff != 0:\n",
    "                            point_end = point_end - rstrip_diff\n",
    "                        entities.append((point_start, point_end + 1 , label))\n",
    "            training_data.append((text, {\"entities\" : entities}))\n",
    "        return training_data\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "    return cleaned_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = trim_entity_spans(convert_dataturks_to_spacy(\"ner.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def clean_dataset(data):\n",
    "    cleanedDF = pd.DataFrame(columns=[\"setences_cleaned\"])\n",
    "    sum1 = 0\n",
    "    for i in tqdm(range(len(data))):\n",
    "        start = 0\n",
    "        emptyList = [\"Empty\"] * len(data[i][0].split())\n",
    "        numberOfWords = 0\n",
    "        lenOfString = len(data[i][0])\n",
    "        strData = data[i][0]\n",
    "        strDictData = data[i][1]\n",
    "        lastIndexOfSpace = strData.rfind(' ')\n",
    "        for i in range(lenOfString):\n",
    "            if (strData[i]==\" \" and strData[i+1]!=\" \"):\n",
    "                for k,v in strDictData.items():\n",
    "                    for j in range(len(v)):\n",
    "                        entList = v[len(v)-j-1]\n",
    "                        if (start>=int(entList[0]) and i<=int(entList[1])):\n",
    "                            emptyList[numberOfWords] = entList[2]\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                start = i + 1  \n",
    "                numberOfWords += 1\n",
    "            if (i == lastIndexOfSpace):\n",
    "                for j in range(len(v)):\n",
    "                        entList = v[len(v)-j-1]\n",
    "                        if (lastIndexOfSpace>=int(entList[0]) and lenOfString<=int(entList[1])):\n",
    "                            emptyList[numberOfWords] = entList[2]\n",
    "                            numberOfWords += 1\n",
    "        cleanedDF = cleanedDF.append(pd.Series([emptyList],  index=cleanedDF.columns ), ignore_index=True )\n",
    "        sum1 = sum1 + numberOfWords\n",
    "    return cleanedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828bbdd7a7994177a3870a054c6c8984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleanedDF = clean_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Afreen Jamadar Active member of IIIT Committee in Third year  Sangli, Maharashtra - Email me on Indeed: indeed.com/r/Afreen-Jamadar/8baf379b705e37c6  I wish to use my knowledge, skills and conceptual understanding to create excellent team environments and work consistently achieving organization objectives believes in taking initiative and work to excellence in my work.  WORK EXPERIENCE  Active member of IIIT Committee in Third year  Cisco Networking -  Kanpur, Uttar Pradesh  organized by Techkriti IIT Kanpur and Azure Skynet. PERSONALLITY TRAITS: â€¢ Quick learning ability â€¢ hard working  EDUCATION  PG-DAC  CDAC ACTS  2017  Bachelor of Engg in Information Technology  Shivaji University Kolhapur -  Kolhapur, Maharashtra  2016  SKILLS  Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT ACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)  ADDITIONAL INFORMATION  TECHNICAL SKILLS:  â€¢ Programming Languages: C, C++, Java, .net, php. â€¢ Web Designing: HTML, XML â€¢ Operating Systems: Windows [â€¦] Windows Server 2003, Linux. â€¢ Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.  https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['content'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['Name', 'Name', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'College Name', 'Empty', 'Empty', 'Degree', 'Degree', 'Degree', 'Degree', 'Degree', 'Empty', 'College Name', 'College Name', 'College Name', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Empty', 'Empty'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedDF.iloc[1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at your cleaned dataset and the categories the named-entities are matched to, or 'tags'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setences_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Name, Name, Designation, Designation, Designa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Name, Name, Empty, Empty, Empty, Empty, Empty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Name, Name, Name, Empty, Empty, Empty, Empty,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Name, Name, Designation, Designation, Designa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Name, Name, Designation, Empty, Companies wor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    setences_cleaned\n",
       "0  [Name, Name, Designation, Designation, Designa...\n",
       "1  [Name, Name, Empty, Empty, Empty, Empty, Empty...\n",
       "2  [Name, Name, Name, Empty, Empty, Empty, Empty,...\n",
       "3  [Name, Name, Designation, Designation, Designa...\n",
       "4  [Name, Name, Designation, Empty, Companies wor..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - Padding and Generating Tags\n",
    "\n",
    "Now, it is time to generate a list of unique tags you will match the named-entities to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = set(cleanedDF['setences_cleaned'].explode().unique())#pd.unique(cleanedDF['setences_cleaned'])#set(tag for doc in cleanedDF['setences_cleaned'].values.tolist() for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Location': 0,\n",
       " 'Skills': 1,\n",
       " 'Email Address': 2,\n",
       " 'Degree': 3,\n",
       " 'Empty': 4,\n",
       " 'Graduation Year': 5,\n",
       " 'Designation': 6,\n",
       " 'Years of Experience': 7,\n",
       " 'College Name': 8,\n",
       " 'UNKNOWN': 9,\n",
       " 'Companies worked at': 10,\n",
       " 'Name': 11}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'College Name',\n",
       " 'Companies worked at',\n",
       " 'Degree',\n",
       " 'Designation',\n",
       " 'Email Address',\n",
       " 'Empty',\n",
       " 'Graduation Year',\n",
       " 'Location',\n",
       " 'Name',\n",
       " 'Skills',\n",
       " 'UNKNOWN',\n",
       " 'Years of Experience'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will create an array of tags from your cleaned dataset. Oftentimes your input sequence will exceed the maximum length of a sequence your network can process. In this case, your sequence will be cut off, and you need to append zeroes onto the end of the shortened sequences using this [Keras padding API](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "labels = cleanedDF['setences_cleaned'].values.tolist()\n",
    "\n",
    "tags = pad_sequences([[tag2id.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2id[\"Empty\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11, 11,  6, ...,  4,  4,  4],\n",
       "       [11, 11,  4, ...,  4,  4,  4],\n",
       "       [11, 11, 11, ...,  4,  1,  4],\n",
       "       ...,\n",
       "       [11, 11,  6, ...,  4,  4,  4],\n",
       "       [11, 11,  6, ...,  4,  4,  4],\n",
       "       [11, 11,  6, ...,  4,  4,  4]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-3'></a>\n",
    "### 1.3 - Tokenize and Align Labels with ðŸ¤— Library\n",
    "\n",
    "Before feeding the texts to a Transformer model, you will need to tokenize your input using a [ðŸ¤— Transformer tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html). It is crucial that the tokenizer you use must match the Transformer model type you are using! In this exercise, you will use the ðŸ¤— [DistilBERT fast tokenizer](https://huggingface.co/transformers/model_doc/distilbert.html), which standardizes the length of your sequence to 512 and pads with zeros. Notice this matches the maximu length you used when creating tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file tokenizer/config.json not found\n",
      "file tokenizer/config.json not found\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast #, TFDistilBertModel\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('tokenizer/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer models are often trained by tokenizers that split words into subwords. For instance, the word 'Africa' might get split into multiple subtokens. This can create some misalignment between the list of tags for the dataset and the list of labels generated by the tokenizer, since the tokenizer can split one word into several, or add special tokens. Before processing, it is important that you align the lists of tags and the list of labels generated by the selected tokenizer with a `tokenize_and_align_labels()` function.\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - tokenize_and_align_labels\n",
    "\n",
    "Implement `tokenize_and_align_labels()`. The function should perform the following:\n",
    "* The tokenizer cuts sequences that exceed the maximum size allowed by your model with the parameter `truncation=True`\n",
    "* Aligns the list of tags and labels with the tokenizer `word_ids` method returns a list that maps the subtokens to the original word in the sentence and special tokens to `None`. \n",
    "* Set the labels of all the special tokens (`None`) to -100 to prevent them from affecting the loss function. \n",
    "* Label of the first subtoken of a word and set the label for the following subtokens to -100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "def tokenize_and_align_labels(tokenizer, examples, tags):\n",
    "    '''\n",
    "    tags: (220, 512), represents the tag integer\n",
    "    Jun's note, what does this function do? The tokenizer can split some words, e.g., the words in index[0] is ~ 255, then after tokenzing, \n",
    "    the list becomes 369, how do we know the \"splitting\" information? Use *.word_is --> it will give a list sth like [0, 0, 0, 1,1, ...]\n",
    "    this list means the first three \"0\"s elements in the tokenized_inputs(assume it's 1D, but in the code it's 2D), the first three elements \n",
    "    are from the original input, index 0. Alternatively,\n",
    "    [input0, input1]\n",
    "    [token0, token1, token2, token3, token4]\n",
    "    \n",
    "    If input0 is split to token0, token1, token2, input1 is split to token3 and token4, then we will see *.is_word will return\n",
    "    [0, 0, 0, 1, 1]\n",
    "    \n",
    "    Using this information, we can also apply the \"splitting\" to our train_y(tags) to align with our training_x, how?\n",
    "    \n",
    "                    label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "    \n",
    "    the new label_ids will keep appending label[word_idx], three times, so if original Y = [100, 101], it will become\n",
    "    [100, 100, 100, 101, 101]\n",
    "    \n",
    "    Vola!\n",
    "    \n",
    "    '''\n",
    "    # convert a sentence to token, note, the len(examples) is counting the char, not the word\n",
    "    # to count words, you may use len(string.strip().split(\" \")). So max_length >> sentence words\n",
    "    tokenized_inputs = tokenizer(examples, truncation=True, is_split_into_words=False, padding='max_length', max_length=512)\n",
    "    labels = []\n",
    "    for i, label in enumerate(tags):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                # this line of code is the key, i think\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have tokenized inputs, you can create train and test datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this function does, (I guess) is that they have used the same number to represent words in \"train_X\", and \"train_Y\" now. Andrew calls this \"tokenize_and_align_labels\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokenize_and_align_labels(tokenizer, df_data['content'].values.tolist(), tags)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    test['input_ids'],\n",
    "    test['labels']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understand what we are doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101, 11113, 24158,  5369,  2243,  1046,  3270,  4646,  2458,\n",
       "        5482,  1011,  9669,  5397,  8191, 14129,  1010, 12092,  1011,\n",
       "       10373,  2033,  2006,  5262,  1024,  5262,  1012,  4012,  1013,\n",
       "        1054,  1013, 11113, 24158,  5369,  2243,  1011,  1046,  3270,\n",
       "        1013,  2184,  2063,  2581,  2050,  2620, 27421,  2581, 16703,\n",
       "        9818, 23777,  2050,  1528,  2000,  2147,  2005,  2019,  3029,\n",
       "        2029,  3640,  2033,  1996,  4495,  2000,  5335,  2026,  4813,\n",
       "        1998,  3716,  2005,  2026,  3265,  1998,  2194,  1005,  1055,\n",
       "        3930,  1999,  2190,  2825,  3971,  1012,  5627,  2000, 20102,\n",
       "        2000,  1024, 14022,  1010, 12092,  2147,  3325,  4646,  2458,\n",
       "        5482,  9669,  5397,  1011,  2281,  2418,  2000,  2556,  2535,\n",
       "        1024,  2747,  2551,  2006, 11834,  1011, 28516,  1012,  4975,\n",
       "        2067, 10497, 14721,  7243, 15794, 10861,  5134,  2005,  1996,\n",
       "       28516,  2029,  2097,  2022, 13330,  2241,  2006,  2445,  7953,\n",
       "        1012,  2036,  1010,  2731,  1996, 28516,  2005,  2367,  2825,\n",
       "       14395, 26755,  1006,  2119,  3893,  1998,  4997,  1007,  1010,\n",
       "        2029,  2097,  2022,  2445,  2004,  7953,  2011,  1996,  5310,\n",
       "        1012,  2495,  1038,  1012,  1041,  1999,  2592,  2671,  1998,\n",
       "        3330,  1038,  1012,  1058,  1012,  1038,  2267,  1997,  3330,\n",
       "        1998,  2974,  1011,  9594,  3669,  1010, 12092,  2257,  2286,\n",
       "        2000,  2238,  2418,  5940,  1999,  5597,  3536, 16765,  2715,\n",
       "        2082,  2258,  2249,  2000,  2233,  2286,  6049,  6358, 13626,\n",
       "        8717,  6819, 25838, 22923,  2258,  2541,  2000,  2233,  2249,\n",
       "        4813,  1039,  1006,  2625,  2084,  1015,  2095,  1007,  1010,\n",
       "        7809,  1006,  2625,  2084,  1015,  2095,  1007,  1010,  7809,\n",
       "        2968,  1006,  2625,  2084,  1015,  2095,  1007,  1010,  7809,\n",
       "        2968,  2291,  1006,  2625,  2084,  1015,  2095,  1007,  1010,\n",
       "        9262,  1006,  2625,  2084,  1015,  2095,  1007,  3176,  2592,\n",
       "        4087,  4813, 16770,  1024,  1013,  1013,  7479,  1012,  5262,\n",
       "        1012,  4012,  1013,  1054,  1013, 11113, 24158,  5369,  2243,\n",
       "        1011,  1046,  3270,  1013,  2184,  2063,  2581,  2050,  2620,\n",
       "       27421,  2581, 16703,  9818, 23777,  2050,  1029,  2003,  3593,\n",
       "        1027, 10151,  1011,  8816,  1004, 20912,  2860,  1027,  8816,\n",
       "        1011,  2327,  1004,  2522,  1027,  1999,  1528,  4730,  2653,\n",
       "        1024,  1039,  1010,  1039,  1009,  1009,  1010,  9262,  1528,\n",
       "       14721,  7243, 15794,  1528,  4274,  1997,  2477,  1528,  3698,\n",
       "        4083,  1528,  7809,  2968,  2291,  1528,  3274,  6125,  1528,\n",
       "        4082,  2291,  2499,  2006,  1024, 11603,  1010,  3645,  1010,\n",
       "        6097,  2512,  1011,  4087,  4813,  1528,  7481,  1998,  2524,\n",
       "        1011,  2551,  1528, 23691,  1998, 12379,  2000,  2367,  8146,\n",
       "        1528, 13205,  1998,  5475,  1528,  2136,  1011,  2447,   102,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test['input_ids'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Abhishek Jha Application Development Associate - Accenture  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  â€¢ To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and company's growth in best possible ways.  Willing to relocate to: Bangalore, Karnataka  WORK EXPERIENCE  Application Development Associate  Accenture -  November 2017 to Present  Role: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input. Also, Training the bot for different possible utterances (Both positive and negative), which will be given as input by the user.  EDUCATION  B.E in Information science and engineering  B.v.b college of engineering and technology -  Hubli, Karnataka  August 2013 to June 2017  12th in Mathematics  Woodbine modern school  April 2011 to March 2013  10th  Kendriya Vidyalaya  April 2001 to March 2011  SKILLS  C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year), Database Management System (Less than 1 year), Java (Less than 1 year)  ADDITIONAL INFORMATION  Technical Skills  https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN   â€¢ Programming language: C, C++, Java â€¢ Oracle PeopleSoft â€¢ Internet Of Things â€¢ Machine Learning â€¢ Database Management System â€¢ Computer Networks â€¢ Operating System worked on: Linux, Windows, Mac  Non - Technical Skills  â€¢ Honest and Hard-Working â€¢ Tolerant and Flexible to Different Situations â€¢ Polite and Calm â€¢ Team-Player\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['content'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 512)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test['input_ids']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 512)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test['labels']).shape#tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dataset.batch(1).as_numpy_iterator())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dataset.batch(1).as_numpy_iterator())[0] == np.array(test['input_ids'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Back to our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 5,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 4,\n",
       " 4,\n",
       " 11,\n",
       " 3,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 11,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 9,\n",
       " 9,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 9,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Star From Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jun's note:\n",
    "\n",
    "After running the above, what we will have?\n",
    "- train_dataset --> tensor slice, tuple (train_X, train_Y)\n",
    "- train_X.shape(200, 512), 200 samples, the 512 indicate # of tokens, note, one word could be split into multiple tokens\n",
    "- Y is indicating which categories the token belong to, if the tokens are from the same word, the corresponding Y for them should be the same. This is alinged by the author Already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Location': 0,\n",
       " 'Skills': 1,\n",
       " 'Email Address': 2,\n",
       " 'Degree': 3,\n",
       " 'Empty': 4,\n",
       " 'Graduation Year': 5,\n",
       " 'Designation': 6,\n",
       " 'Years of Experience': 7,\n",
       " 'College Name': 8,\n",
       " 'UNKNOWN': 9,\n",
       " 'Companies worked at': 10,\n",
       " 'Name': 11}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-4'></a>\n",
    "### 1.4 - Optimization\n",
    "\n",
    "Fantastic! Now you can finally feed your data into into a pretrained ðŸ¤— model. You will optimize a DistilBERT model, which matches the tokenizer you used to preprocess your data. Try playing around with the different hyperparamters to improve your results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data from Website and with config (Andrew's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForTokenClassification: ['vocab_layer_norm', 'vocab_transform', 'vocab_projector', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_193', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertForTokenClassification\n",
    "\n",
    "#model = TFDistilBertForTokenClassification.from_pretrained('model/', num_labels=len(unique_tags))\n",
    "model = TFDistilBertForTokenClassification.from_pretrained('distilbert-base-uncased', num_labels=len(unique_tags), config='model/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "14/14 [==============================] - 260s 18s/step - loss: 1.7163 - accuracy: 0.6170\n",
      "Epoch 2/3\n",
      "14/14 [==============================] - 203s 14s/step - loss: 0.5937 - accuracy: 0.7537\n",
      "Epoch 3/3\n",
      "14/14 [==============================] - 188s 13s/step - loss: 0.5154 - accuracy: 0.7537\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy']) # can also use any keras loss fn\n",
    "history = model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at my_model.h5 were not used when initializing TFDistilBertForTokenClassification: ['dropout_193']\n",
      "- This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at my_model.h5 and are newly initialized: ['dropout_233']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForTokenClassification.from_pretrained('my_model.h5', num_labels=len(unique_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_token_classification_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "dropout_233 (Dropout)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  9228      \n",
      "=================================================================\n",
      "Total params: 66,372,108\n",
      "Trainable params: 66,372,108\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make Only Classifier Layer Trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_trainable = False\n",
    "for layer in model.layers:\n",
    "    if layer.name == 'classifier':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_token_classification_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "dropout_233 (Dropout)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  9228      \n",
      "=================================================================\n",
      "Total params: 66,372,108\n",
      "Trainable params: 9,228\n",
      "Non-trainable params: 66,362,880\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train only the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "14/14 [==============================] - 76s 5s/step - loss: 0.5010 - accuracy: 0.7537\n",
      "Epoch 2/30\n",
      "14/14 [==============================] - 71s 5s/step - loss: 0.5004 - accuracy: 0.7537\n",
      "Epoch 3/30\n",
      "14/14 [==============================] - 69s 5s/step - loss: 0.5003 - accuracy: 0.7537\n",
      "Epoch 4/30\n",
      "14/14 [==============================] - 71s 5s/step - loss: 0.4955 - accuracy: 0.7537\n",
      "Epoch 5/30\n",
      "14/14 [==============================] - 66s 5s/step - loss: 0.4955 - accuracy: 0.7537\n",
      "Epoch 6/30\n",
      "14/14 [==============================] - 65s 5s/step - loss: 0.4959 - accuracy: 0.7537\n",
      "Epoch 7/30\n",
      "14/14 [==============================] - 73s 5s/step - loss: 0.4976 - accuracy: 0.7537\n",
      "Epoch 8/30\n",
      "14/14 [==============================] - 69s 5s/step - loss: 0.4967 - accuracy: 0.7537\n",
      "Epoch 9/30\n",
      "14/14 [==============================] - 68s 5s/step - loss: 0.4930 - accuracy: 0.7537\n",
      "Epoch 10/30\n",
      "14/14 [==============================] - 67s 5s/step - loss: 0.4919 - accuracy: 0.7537\n",
      "Epoch 11/30\n",
      "14/14 [==============================] - 67s 5s/step - loss: 0.4895 - accuracy: 0.7537\n",
      "Epoch 12/30\n",
      "14/14 [==============================] - 68s 5s/step - loss: 0.4913 - accuracy: 0.7537\n",
      "Epoch 13/30\n",
      "14/14 [==============================] - 68s 5s/step - loss: 0.4934 - accuracy: 0.7537\n",
      "Epoch 14/30\n",
      "14/14 [==============================] - 68s 5s/step - loss: 0.4888 - accuracy: 0.7537\n",
      "Epoch 15/30\n",
      "14/14 [==============================] - 70s 5s/step - loss: 0.4885 - accuracy: 0.7537\n",
      "Epoch 16/30\n",
      "14/14 [==============================] - 69s 5s/step - loss: 0.4866 - accuracy: 0.7537\n",
      "Epoch 17/30\n",
      "14/14 [==============================] - 68s 5s/step - loss: 0.4887 - accuracy: 0.7537\n",
      "Epoch 18/30\n",
      " 8/14 [================>.............] - ETA: 28s - loss: 0.4852 - accuracy: 0.7652"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l3/390_64zs7_3c075928zrlr2w0000gn/T/ipykernel_91005/3148509807.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# can also use any keras loss fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/workspace/ML_coursera_jupyter/env/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/workspace/ML_coursera_jupyter/env/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/workspace/ML_coursera_jupyter/env/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/workspace/ML_coursera_jupyter/env/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/workspace/ML_coursera_jupyter/env/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/Dropbox/workspace/ML_coursera_jupyter/env/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/workspace/ML_coursera_jupyter/env/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy']) # can also use any keras loss fn\n",
    "history = model.fit(train_dataset.shuffle(1000).batch(16), epochs=30, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "#### Here's what you should remember\n",
    "\n",
    "- Named-entity recognition (NER) detects and classifies named-entities, and can help process resumes, customer reviews, browsing histories, etc. \n",
    "- You must preprocess text data with the corresponding tokenizer to the pretrained model before feeding your input into your Transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix (Nothing Useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Data Input Example - from_tensor_slices\n",
    "\n",
    "print(type(train_dataset))\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    " \n",
    "features, labels = (np.random.sample((6, 3)),  # æ¨¡æ‹Ÿ6ç»„æ•°æ®ï¼Œæ¯ç»„æ•°æ®3ä¸ªç‰¹å¾\n",
    "                    np.random.sample((6, 1)))  # æ¨¡æ‹Ÿ6ç»„æ•°æ®ï¼Œæ¯ç»„æ•°æ®å¯¹åº”ä¸€ä¸ªæ ‡ç­¾ï¼Œæ³¨æ„ä¸¤è€…çš„ç»´æ•°å¿…é¡»åŒ¹é…\n",
    " \n",
    "print((features, labels))  #  è¾“å‡ºä¸‹ç»„åˆçš„æ•°æ®\n",
    "data = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "print(data)  # è¾“å‡ºå¼ é‡çš„ä¿¡æ¯\n",
    "\n",
    "ts = tf.constant([[1, 2], [3, 4]])\n",
    "ds = tf.data.Dataset.from_tensor_slices(ts)   # [1, 2], [3, 4]\n",
    "\n",
    "for i in ds:\n",
    "    print(i.numpy())\n",
    "print(ds.batch(2)) # can select batch size\n",
    "print(\"--------\")\n",
    "print(next(ds.batch(2).as_numpy_iterator()))\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "t = tf.range(10.)[:, None]\n",
    "print(t)\n",
    "print(t.shape)\n",
    "print(\"-----\")\n",
    "t = tf.data.Dataset.from_tensor_slices(t)\n",
    "#<TensorSliceDataset shapes: (1,), types: tf.float32>\n",
    "for i in t:\n",
    "    print(i.numpy())\n",
    "\n",
    "next(train_dataset.batch(1).as_numpy_iterator())[0].shape\n",
    "\n",
    "next(train_dataset.batch(1).as_numpy_iterator())[1]\n",
    "\n",
    "next(train_dataset.batch(1).as_numpy_iterator())[1].shape\n",
    "\n",
    "##### Back to our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4602dd619744ce4b1e4268f8bd9956b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing TFBertForTokenClassification: ['dropout_147']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e4b1bdea1a4f4fa78fcc313da80fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263edd50f2684d6c800246025a1d8fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d90d85961ac4612b811f0044eba8a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58238d308a5451d9803d763c782e9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TFAutoModelForTokenClassification, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, \" \\\n",
    "           \"therefore very close to the Manhattan Bridge.\"\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"tf\")\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "outputs = model(**inputs)[0]\n",
    "predictions = tf.argmax(outputs, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 32), dtype=int64, numpy=\n",
       "array([[0, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 8, 8, 8, 0, 0, 0, 0, 0, 8, 8, 8,\n",
       "        0, 0, 0, 0, 0, 0, 8, 8, 0, 0]])>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "243.807px",
    "width": "251.989px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "685.696px",
    "left": "0px",
    "right": "1533.47px",
    "top": "110.284px",
    "width": "211.989px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

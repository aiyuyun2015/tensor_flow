{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "residential-battery",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev4 toc-item\"><a href=\"#Some-Tests-for-backend-functions\" data-toc-modified-id=\"Some-Tests-for-backend-functions-0001\"><span class=\"toc-item-num\">0.0.0.1&nbsp;&nbsp;</span>Some Tests for backend functions</a></div><div class=\"lev4 toc-item\"><a href=\"#kears.backend.permute_dimensions\" data-toc-modified-id=\"kears.backend.permute_dimensions-0002\"><span class=\"toc-item-num\">0.0.0.2&nbsp;&nbsp;</span>kears.backend.permute_dimensions</a></div><div class=\"lev3 toc-item\"><a href=\"#Continue-our-example\" data-toc-modified-id=\"Continue-our-example-001\"><span class=\"toc-item-num\">0.0.1&nbsp;&nbsp;</span>Continue our example</a></div><div class=\"lev3 toc-item\"><a href=\"#Define-the-Final-Loss\" data-toc-modified-id=\"Define-the-Final-Loss-002\"><span class=\"toc-item-num\">0.0.2&nbsp;&nbsp;</span>Define the Final Loss</a></div><div class=\"lev4 toc-item\"><a href=\"#Setting-up-the-gradient-descent-process\" data-toc-modified-id=\"Setting-up-the-gradient-descent-process-0021\"><span class=\"toc-item-num\">0.0.2.1&nbsp;&nbsp;</span>Setting up the gradient-descent process</a></div><div class=\"lev4 toc-item\"><a href=\"#Loop\" data-toc-modified-id=\"Loop-0022\"><span class=\"toc-item-num\">0.0.2.2&nbsp;&nbsp;</span>Loop</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "worst-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "individual-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "subsequent-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "input_path = 'nueral_transfer_images'\n",
    "\n",
    "target_image_path = os.path.join(input_path, 'portrait.jpg')\n",
    "style_reference_image_path = os.path.join(input_path, 'transfer_style_reference.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "invalid-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = load_img(target_image_path).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "legendary-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eastern-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = int(width * img_height / height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adolescent-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications import vgg19\n",
    "# Check out https://stackoverflow.com/questions/47555829/preprocess-input-method-in-keras\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_height, img_width)) # This loads an image and resizes the image to (400, scaled height)\n",
    "    img = img_to_array(img) # input is PIL.Image.Image type, converet to (400, scaled height, 3)\n",
    "    img = np.expand_dims(img, axis=0) # add the number of images: x.shape = (1, 224, 224, 3), so here batc_size = 1 is added\n",
    "    \n",
    "    # preprocess_input subtracts the mean RGB channels of the imagenet dataset. \n",
    "    # This is because the model you are using has been trained on a \n",
    "    # different dataset: x.shape is still (1, 224, 224, 3)\n",
    "    img = vgg19.preprocess_input(img) \n",
    "\n",
    "    return img\n",
    "\n",
    "# VGG networks are trained on the image with each channel normalized by mean = [103.939, 116.779, 123.68]and with channels BGR. Furthermore, \n",
    "# since our optimized image may take its values anywhere between −∞ and ∞ , we must clip to maintain our values from within the 0-255 range.\n",
    "def deprocess_image(x):\n",
    "    \n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "partial-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# target_image and style_reference_image will be static\n",
    "target_image = K.constant(preprocess_image(target_image_path))\n",
    "style_reference_image = K.constant(preprocess_image(style_reference_image_path))\n",
    "\n",
    "# Note, we will vary combination_image (the output), by gradient descent, thus it's\n",
    "# a placeholder\n",
    "combination_image = K.placeholder((1, \n",
    "                                   img_height, \n",
    "                                   img_width, 3))\n",
    "\n",
    "input_tensor = K.concatenate([target_image, \n",
    "                              style_reference_image, \n",
    "                              combination_image], axis=0)\n",
    "\n",
    "model = vgg19.VGG19(input_tensor=input_tensor,\n",
    "                    weights='imagenet',\n",
    "                    include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-hepatitis",
   "metadata": {},
   "source": [
    "$$J_{content}(C,G) =  \\frac{1}{4 \\times n_H \\times n_W \\times n_C}\\sum _{ \\text{all entries}} (a^{(C)} - a^{(G)})^2\\tag{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "manual-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    # K.square --> elementwise square\n",
    "    # K.sum --> sum all the entries\n",
    "    # where is your coefficient?? 4*n_H * n_W * n_C\n",
    "    return K.sum(K.square(combination - base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-chambers",
   "metadata": {},
   "source": [
    "#### Some Tests for backend functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-mainstream",
   "metadata": {},
   "source": [
    "##### keras.backend.batch_flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "blank-stewart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input Tensor(\"Const_34:0\", shape=(2, 2, 2), dtype=float32) \n",
      "\n",
      "batch_flatten Tensor(\"Reshape_20:0\", shape=(2, 4), dtype=float32) \n",
      "\n",
      "flatten Tensor(\"Reshape_21:0\", shape=(8,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# turn a tensor into flat tensor (1D), however, this is different from flatten\n",
    "# batch_flatten will keep the first dimension(the batch dimension)\n",
    "\n",
    "a = tf.constant([[[1.0, 1.5], [2.0, 2.5]], [[3.0, 3.5], [4.0, 4.5]]])\n",
    "print('input', a, '\\n')\n",
    "b = K.batch_flatten(a)\n",
    "print('batch_flatten', b, '\\n')\n",
    "print('flatten', K.flatten(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-going",
   "metadata": {},
   "source": [
    "#### kears.backend.permute_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interpreted-theory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape: (2, 3, 4)\n",
      "[[[ 1  2  3  4]\n",
      "  [ 5  6  7  8]\n",
      "  [ 9 10 11 12]]\n",
      "\n",
      " [[13 14 15 16]\n",
      "  [17 18 19 20]\n",
      "  [21 22 23 24]]]\n",
      "===================\n",
      "x shape (3, 2, 4)\n",
      "[[[ 1.  2.  3.  4.]\n",
      "  [13. 14. 15. 16.]]\n",
      "\n",
      " [[ 5.  6.  7.  8.]\n",
      "  [17. 18. 19. 20.]]\n",
      "\n",
      " [[ 9. 10. 11. 12.]\n",
      "  [21. 22. 23. 24.]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "\n",
    "a = np.arange(1, 25).reshape((2, 3, 4))\n",
    "print(\"a shape:\", a.shape)\n",
    "print(a)\n",
    "print('===================')\n",
    "b = K.constant(a)\n",
    "x = K.permute_dimensions(b, (1, 0, 2))\n",
    "\n",
    "\n",
    "print(\"x shape\", x.numpy().shape)\n",
    "print(x.numpy())\n",
    "\n",
    "#print(x1.numpy())\n",
    "\n",
    "\n",
    "#run below for tf1 (no eager execution), otherwise, just pull out numpy() value\n",
    "#with K.get_session() as ss:\n",
    "#    print(ss.run(x))\n",
    "#     print('-----')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "outside-athens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  5.,  9.],\n",
       "        [13., 17., 21.]],\n",
       "\n",
       "       [[ 2.,  6., 10.],\n",
       "        [14., 18., 22.]],\n",
       "\n",
       "       [[ 3.,  7., 11.],\n",
       "        [15., 19., 23.]],\n",
       "\n",
       "       [[ 4.,  8., 12.],\n",
       "        [16., 20., 24.]]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "silver-retailer",
   "metadata": {},
   "source": [
    "### Continue our example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-christopher",
   "metadata": {},
   "source": [
    "<img src=\"images/NST_GM.png\" style=\"width:900px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "intermediate-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    # K.batch_flatten --> keep the first axis=0 same dimension, flat the rest\n",
    "    # K.permute_dimensions --> switch the order of the tensor indices\n",
    "    # This means, generate a matrix like above.\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-outside",
   "metadata": {},
   "source": [
    "$$J_{style}^{[l]}(S,G) = \\frac{1}{4 \\times {n_C}^2 \\times (n_H \\times n_W)^2} \\sum _{i=1}^{n_C}\\sum_{j=1}^{n_C}(G^{(S)}_{ij} - G^{(G)}_{ij})^2\\tag{2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "induced-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_height * img_width\n",
    "    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-headline",
   "metadata": {},
   "source": [
    "To these two loss components, you add a third: the total variation loss, which operates on the pixels of the generated combination image. It encourages spatial continuity in the generated image, thus avoiding overly pixelated results. You can interpret it as a regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "touched-institute",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    a = K.square(\n",
    "        x[:, :img_height - 1, :img_width - 1, :] -\n",
    "        x[:, 1:, :img_width - 1, :])\n",
    "    b = K.square(\n",
    "        x[:, :img_height - 1, :img_width - 1, :] -\n",
    "        x[:, :img_height - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-elements",
   "metadata": {},
   "source": [
    "### Define the Final Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "alone-oxide",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(3, 400, 485, 3)]        0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (3, 400, 485, 64)         1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (3, 400, 485, 64)         36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (3, 200, 242, 64)         0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (3, 200, 242, 128)        73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (3, 200, 242, 128)        147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (3, 100, 121, 128)        0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (3, 100, 121, 256)        295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (3, 100, 121, 256)        590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (3, 100, 121, 256)        590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (3, 100, 121, 256)        590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (3, 50, 60, 256)          0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (3, 50, 60, 512)          1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (3, 50, 60, 512)          2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (3, 50, 60, 512)          2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (3, 50, 60, 512)          2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (3, 25, 30, 512)          0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (3, 25, 30, 512)          2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (3, 25, 30, 512)          2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (3, 25, 30, 512)          2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (3, 25, 30, 512)          2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (3, 12, 15, 512)          0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "corrected-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "content_layer = 'block5_conv2'\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1',\n",
    "                'block5_conv1']\n",
    "total_variation_weight = 1e-4\n",
    "style_weight = 1.\n",
    "content_weight = 0.025\n",
    "loss = K.variable(0.)\n",
    "layer_features = outputs_dict[content_layer]\n",
    "target_image_features = layer_features[0, :, :, :] # block5_conv has shape  (3, 25, 30, 512) \n",
    "combination_features = layer_features[2, :, :, :] # the input is (target, ref, combination), thus index 0 is target activation, index 2 is combination activation\n",
    "\n",
    "# content_loss\n",
    "loss = loss + content_weight * content_loss(target_image_features, combination_features)\n",
    "\n",
    "# style_loss\n",
    "for layer_name in style_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :] # layer_features[1:] is the reference activation\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    sl = style_loss(style_reference_features, combination_features)\n",
    "    loss += (style_weight / len(style_layers)) * sl\n",
    "\n",
    "# add a total varaition loss, as a regulariztion?\n",
    "loss += total_variation_weight * total_variation_loss(combination_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "developed-township",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_7:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-protein",
   "metadata": {},
   "source": [
    "#### Setting up the gradient-descent process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "committed-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "prescribed-local",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = K.gradients(loss, combination_image)[0] # output is a list of 0 element, so get it..\n",
    "# combination_image is the placeholder, our output\n",
    "\n",
    "fetch_loss_and_grads = K.function([combination_image], [loss, grads]) # first argument is input, second argunment are the outputs\n",
    "# thus, fetch_loss_and_grads will be a tuple, that include the loss, grads, given combination_image\n",
    "\n",
    "# Just two functions.. loss and grads, need for fmin_l_bfgs_b, fancier way.. use class\n",
    "class Evaluator(object):\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "        \n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        x = x.reshape((1, img_height, img_width, 3))\n",
    "        outs = fetch_loss_and_grads([x])\n",
    "        loss_value = outs[0]\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "    \n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-proposal",
   "metadata": {},
   "source": [
    "#### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "creative-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from keras.preprocessing.image import save_img\n",
    "#from scipy.misc import imsave # bug\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "married-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_prefix = 'my_result'\n",
    "iterations = 20\n",
    "\n",
    "# path of portrait\n",
    "# Preprocessed numpy.array or a tf.Tensor with type float32.\n",
    "# The images are converted from RGB to BGR, then each color channel is zero-centered with respect to the ImageNet dataset, without scaling.\n",
    "\n",
    "x = preprocess_image(target_image_path) \n",
    "print(x.shape)\n",
    "x = x.flatten()\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/scipy/scipy/blob/v1.7.1/scipy/optimize/lbfgsb.py#L48-L207\n",
    "# fmin_1_bfgs_b():\n",
    "#     func : callable f(x,*args)\n",
    "#         Function to minimize.\n",
    "#     x0 : ndarray\n",
    "#         Initial guess.\n",
    "#     fprime : callable fprime(x,*args), optional\n",
    "#         The gradient of `func`. If None, then `func` returns the function\n",
    "#         value and the gradient (``f, g = func(x, *args)``), unless\n",
    "#         `approx_grad` is True in which case `func` returns only ``f``.\n",
    "#    maxfun : int, optional\n",
    "#        Maximum number of function evaluations.\n",
    "# ...\n",
    "#    Returns\n",
    "#     -------\n",
    "#     x : array_like\n",
    "#         Estimated position of the minimum.\n",
    "#     f : float\n",
    "#         Value of `func` at the minimum.\n",
    "#     d : dict\n",
    "#         Information dictionary.\n",
    "#         * d['warnflag'] is\n",
    "#           - 0 if converged,\n",
    "#           - 1 if too many function evaluations or too many iterations,\n",
    "#           - 2 if stopped for another reason, given in d['task']\n",
    "#         * d['grad'] is the gradient at the minimum (should be 0 ish)\n",
    "#         * d['funcalls'] is the number of function calls made.\n",
    "#         * d['nit'] is the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-logan",
   "metadata": {},
   "source": [
    "##### Example to illustrate fmin_1_bfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "statewide-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y(x):\n",
    "    return x**2 - 2*x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "compatible-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_prime(x):\n",
    "    return 2*x -2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "occupational-score",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.]),\n",
       " array([2.]),\n",
       " {'grad': array([0.]),\n",
       "  'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'funcalls': 2,\n",
       "  'nit': 1,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmin_l_bfgs_b(y, 0, fprime=y_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-penetration",
   "metadata": {},
   "source": [
    "##### Continue Our Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dominican-residence",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of iteration 0\n",
      "Current loss value: 5671027000.0\n",
      "Image saved as my_result_at_iteration_0.png\n",
      "Iteration 0 completed in 89s\n",
      "Start of iteration 1\n",
      "Current loss value: 1736448000.0\n",
      "Image saved as my_result_at_iteration_1.png\n",
      "Iteration 1 completed in 82s\n",
      "Start of iteration 2\n",
      "Current loss value: 979843400.0\n",
      "Image saved as my_result_at_iteration_2.png\n",
      "Iteration 2 completed in 78s\n",
      "Start of iteration 3\n",
      "Current loss value: 711555700.0\n",
      "Image saved as my_result_at_iteration_3.png\n",
      "Iteration 3 completed in 82s\n",
      "Start of iteration 4\n",
      "Current loss value: 562097800.0\n",
      "Image saved as my_result_at_iteration_4.png\n",
      "Iteration 4 completed in 81s\n",
      "Start of iteration 5\n",
      "Current loss value: 482284100.0\n",
      "Image saved as my_result_at_iteration_5.png\n",
      "Iteration 5 completed in 83s\n",
      "Start of iteration 6\n",
      "Current loss value: 423983520.0\n",
      "Image saved as my_result_at_iteration_6.png\n",
      "Iteration 6 completed in 83s\n",
      "Start of iteration 7\n",
      "Current loss value: 376935360.0\n",
      "Image saved as my_result_at_iteration_7.png\n",
      "Iteration 7 completed in 81s\n",
      "Start of iteration 8\n",
      "Current loss value: 344981600.0\n",
      "Image saved as my_result_at_iteration_8.png\n",
      "Iteration 8 completed in 80s\n",
      "Start of iteration 9\n",
      "Current loss value: 324482850.0\n",
      "Image saved as my_result_at_iteration_9.png\n",
      "Iteration 9 completed in 81s\n",
      "Start of iteration 10\n",
      "Current loss value: 300088030.0\n",
      "Image saved as my_result_at_iteration_10.png\n",
      "Iteration 10 completed in 82s\n",
      "Start of iteration 11\n",
      "Current loss value: 277566980.0\n",
      "Image saved as my_result_at_iteration_11.png\n",
      "Iteration 11 completed in 82s\n",
      "Start of iteration 12\n",
      "Current loss value: 264823940.0\n",
      "Image saved as my_result_at_iteration_12.png\n",
      "Iteration 12 completed in 81s\n",
      "Start of iteration 13\n",
      "Current loss value: 253272220.0\n",
      "Image saved as my_result_at_iteration_13.png\n",
      "Iteration 13 completed in 83s\n",
      "Start of iteration 14\n",
      "Current loss value: 240812620.0\n",
      "Image saved as my_result_at_iteration_14.png\n",
      "Iteration 14 completed in 81s\n",
      "Start of iteration 15\n",
      "Current loss value: 231461660.0\n",
      "Image saved as my_result_at_iteration_15.png\n",
      "Iteration 15 completed in 82s\n",
      "Start of iteration 16\n",
      "Current loss value: 219656270.0\n",
      "Image saved as my_result_at_iteration_16.png\n",
      "Iteration 16 completed in 84s\n",
      "Start of iteration 17\n",
      "Current loss value: 210514400.0\n",
      "Image saved as my_result_at_iteration_17.png\n",
      "Iteration 17 completed in 80s\n",
      "Start of iteration 18\n",
      "Current loss value: 202924020.0\n",
      "Image saved as my_result_at_iteration_18.png\n",
      "Iteration 18 completed in 73s\n",
      "Start of iteration 19\n",
      "Current loss value: 197014380.0\n",
      "Image saved as my_result_at_iteration_19.png\n",
      "Iteration 19 completed in 74s\n"
     ]
    }
   ],
   "source": [
    "for i in range(iterations):\n",
    "    print('Start of iteration', i)\n",
    "    start_time = time.time()\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, # the function to minimize\n",
    "                                     x, # initial guess of input\n",
    "                                     fprime=evaluator.grads, # function's grades\n",
    "                                     maxfun=20)\n",
    "    print('Current loss value:', min_val)\n",
    "    img = x.copy().reshape((img_height, img_width, 3))\n",
    "    img = deprocess_image(img)\n",
    "    fname = result_prefix + '_at_iteration_%d.png' % i\n",
    "    \n",
    "    # Save image in each iteration, file name is \"fname\"\n",
    "    save_img(fname, img)\n",
    "    print('Image saved as', fname)\n",
    "    end_time = time.time()\n",
    "    print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-engagement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "135px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "761px",
    "left": "0px",
    "right": "1708px",
    "top": "110px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
